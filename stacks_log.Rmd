---
title: "Stacks Log"
output: html_notebook
---

### Summary

Does cryptic genetic variation contribute to the adaptive divergence of freshwater populations of threespine stickleback? Let's get that genomic data to start figuring it out.

This log describes the process of genotyping 2600 stickleback from raw illumina hiseq reads using Stacks v2

### Server Directory

plastic #top directory
	seq_data #all sequencing data
	stacks #all input (except raw sequence data) and output of stacks
		info #all pop_maps
		alignments #all alignments (sorted bam  files output from bowtie2)
		genome #bowtie indices and reference genome
		genotypes #output of gstacks and populations
		slurm #all stacks slurm jobs
		cleaned #cleaned radtags
		
##### seqdata
for processradtags each set of PE data needs to be in its own directory, write shell script to accomplish this


```{bash}
for f in *.fastq.gz; do
    name=`echo "$f"|sed 's/_R[12]_001.fastq.gz//'`
    dir="$name"
    mkdir -p "$dir"
    mv "$f" "$dir"
done
```



### Fish metadata
##### Barcodes

Used a python script to split the file 'master_barcode_key.txt' into separate files for each lane.

```{python barcode_splitter, python.reticulate = FALSE}

""" The input file has four columns, this script takes writes columns 2 and 3 (barcode and individual) to a new file based on the value of column 4."""

import csv

with open('/Users/ddayan/Science/plasticity/analysis/bioinformatics/metadata/master_barcode_key.csv') as fin:    
    csvin = csv.DictReader(fin)
    # Category -> open file lookup
    outputs = {}
    for row in csvin:
        cat = row['library']
        # Open a new file and write the header
        if cat not in outputs:
            fout = open('{}.csv'.format(cat), 'w')
            dw = csv.DictWriter(fout, fieldnames=csvin.fieldnames)
            dw.writeheader()
            outputs[cat] = fout, dw
        # Always write the row
        outputs[cat][1].writerow(row)
    # Close all the files
    for fout, _ in outputs.values():
        fout.close()

```

oops, only meant to keep columns 2 and 3 and need to write to tab delimited file

```{bash}

for i in ./*csv
do
  cut -d "," -f 2,3 $i > ${i%.csv}.tmp
done


for i in ./*tmp
do
    tr "," "\\t" < $i > ${i%.tmp}_barcodes.txt
done


```


##### Popmaps
### process radtags

Process radtags ran with the following options:
-P paired
-c remove any read with an uncalled base
-q remove any read with low quality
-r rescue barcodes



example slurm script for process radtags, ran individually for all lanes using sbatch in directory plastic/stacks/cleaned

```{bash}
#!/bin/bash

# set the number of nodes
#SBATCH --nodes=1

# set max wall-clock time (D-HH:MM:SS)
#SBATCH --time=0-23:59:00

# set partition/queue to use
#SBATCH --partition=day-long-gpu

# set name of job
#SBATCH --job-name=library_4_process_radtags

# set name of output file
#SBATCH --output=library_4_process_radtags.out

# mail alert at start, end and abortion of execution
#SBATCH --mail-type=ALL

# send mail to this address
#SBATCH --mail-user=ddayan@clarku.edu

source /opt/stacks-2.3/bin/source_me
/opt/stacks-2.3/bin/process_radtags -P  -p ../../seq_data/run2/pls-4 -b ../info/library_4_barcodes.txt -o ./ -e pstI --inline-null -c -q -r --adapter-1 GATCGGAAGAGCGGTTCAGCAGGAATGCCGAGACCGATCAGAACAA --adapter-2 AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCAT --adapter_mm 2 &> pr_library_4.oe
```

same using a job array
```{bash}
#!/bin/bash

# set the number of nodes
#SBATCH --nodes=1

# set max wall-clock time (D-HH:MM:SS)
#SBATCH --time=6-23:59:00

# set partition/queue to use
#SBATCH --partition=week-long-cpu

# set name of output file
#SBATCH --output=library_%a_process_radtags.out

# mail alert at start, end and abortion of execution
#SBATCH --mail-type=ALL

# send mail to this address
#SBATCH --mail-user=ddayan@clarku.edu

#set the array
#SBATCH --array=9,10,11,12 

# set name of job
#SBATCH --job-name=library_%a_process_radtags

source /opt/stacks-2.3/bin/source_me
/opt/stacks-2.3/bin/process_radtags -P  -p ../../seq_data/run2/pls-${SLURM_ARRAY_TASK_ID} -b ../info/library_${SLURM_ARRAY_TASK_ID}_barcodes.txt -o ./ -e pstI --inline-null -c -q -r --adapter-1 GATCGGAAGAGCGGTTCAGCAGGAATGCCGAGACCGATCAGAACAA --adapter-2 AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCAT --adapter_mm 2 &> pr_library_${SLURM_ARRAY_TASK_ID}.oe
```

after process_radtags the four sets of reads (forward reverse and forward and reverse remainder reads) can be concatenated into a single file, however if using gstacks in the paired end mode these reads will not be included so it can be skipped for PE analysis
```{bash}
For those of you that are looking for a quick way to concatenate your samples, you could try this:

Suppose you have a text file named "indiv" that has the names if your samples, and your FASTQ files are in a directory named "samples":

$ ls
indiv  samples
$ mkdir samples_concat
for i in `cat indiv`; do cat samples/$i* > samples_concat/$i.fq.gz; done

Or, if you already have your population assignment file 'pops':

$ ls
pops  samples
$ mkdir samples_concat
$ for i in `awk '{print $1}' pops`; do cat samples/$i* > samples_concat/$i.fq.gz; done


```

### alignment

after raw reads go through QC and are demultiplexed and concatenated they are aligned to the genome
alignment used BWA-mem with default options against the broad_s1 g aculeautus genome downloaded from ensembl

two slurm scripts are below for bwa mem alignments: single lane and batch submission

```{bash}
#!/bin/bash

# set max wall-clock time (D-HH:MM:SS)
#SBATCH --time=6-23:59:00

#SBATCH --cpus-per-task=38

# set partition/queue to use
#SBATCH --partition=week-long-cpu

# set name of job
#SBATCH --job-name=bwa_default

# set name of output file
#SBATCH --output=bwadefault.out

# send mail to this address
#SBATCH --mail-user=ddayan@clarku.edu

source /opt/samtools-1.6/source_me
files="bb_f_10
bb_f_110
bb_f_119
bb_f_121
bb_f_129
bb_f_131
bb_f_157
bb_f_161
bb_f_165
bb_f_168
bb_f_170
bb_f_174
bb_f_181
bb_f_183
bb_f_186
bb_f_189
bb_f_19
bb_f_191
bb_f_193
bb_f_20
bb_f_213
bb_f_218
bb_f_221
bb_f_231
bb_f_236
bb_f_237
bb_f_239
bb_f_24
bb_f_282
bb_f_288
bb_f_316
bb_f_320
bb_f_4
bb_f_41
bb_f_43
bb_f_46
bb_f_5
bb_f_56
bb_f_66
bb_f_82
bb_f_99
bb_s_1
bb_s_104
bb_s_108
bb_s_114
bb_s_116
bb_s_126
bb_s_13
bb_s_133
bb_s_135
bb_s_141
bb_s_143
bb_s_151
bb_s_153
bb_s_159
bb_s_167
bb_s_170
bb_s_172
bb_s_175
bb_s_18
bb_s_185
bb_s_193
bb_s_20
bb_s_204
bb_s_205
bb_s_206
bb_s_207
bb_s_209
bb_s_210
bb_s_216
bb_s_220
bb_s_225
bb_s_237
bb_s_241
bb_s_246
bb_s_250
bb_s_256
bb_s_262
bb_s_273
bb_s_277
bb_s_283
bb_s_284
bb_s_287
bb_s_292
bb_s_300
bb_s_311
bb_s_314
bb_s_321
bb_s_45
bb_s_68
bb_s_75
bb_s_77
bb_s_81
bb_s_86
bb_s_89
cl_f_119
cl_f_12
cl_f_121
cl_f_126
cl_f_139
cl_f_141
cl_f_142
cl_f_146
cl_f_148
cl_f_154
cl_f_155
cl_f_159
cl_f_175
cl_f_177
cl_f_187
cl_f_192
cl_f_197
cl_f_20
cl_f_21
cl_f_214
cl_f_226
cl_f_234
cl_f_236
cl_f_25
cl_f_260
cl_f_265
cl_f_269
cl_f_270
cl_f_272
cl_f_276
cl_f_286
cl_f_288
cl_f_296
cl_f_312
cl_f_314
cl_f_34
cl_f_343
cl_f_348
cl_f_38
cl_f_41
cl_f_46
cl_f_52
cl_f_53
cl_f_55
cl_f_56
cl_f_68
cl_f_76
cl_f_79
cl_f_85
cl_f_88
cl_s_104
cl_s_110
cl_s_113
cl_s_115
cl_s_117
cl_s_119
cl_s_129
cl_s_142
cl_s_147
cl_s_148
cl_s_154
cl_s_17
cl_s_179
cl_s_188
cl_s_189
cl_s_19
cl_s_190
cl_s_199
cl_s_20
cl_s_215
cl_s_23
cl_s_234
cl_s_236
cl_s_249
cl_s_257
cl_s_26
cl_s_27
cl_s_284
cl_s_30
cl_s_308
cl_s_310
cl_s_32
cl_s_34
cl_s_35
cl_s_37
cl_s_40
cl_s_43
cl_s_47
cl_s_57
cl_s_58
cl_s_71
cl_s_77
cl_s_8
cl_s_9
cl_s_96
lb_f_100
lb_f_109
lb_f_114
lb_f_119
lb_f_134
lb_f_137
lb_f_142
lb_f_143
lb_f_146
lb_f_150
lb_f_151
lb_f_172
lb_f_178
lb_f_179
lb_f_185
lb_f_191
lb_f_197
lb_f_202
lb_f_206
lb_f_214
lb_f_220
lb_f_226
lb_f_230
lb_f_232
lb_f_237
lb_f_239
lb_f_251
lb_f_253
lb_f_255
lb_f_256
lb_f_275
lb_f_284
lb_f_305
lb_f_49
lb_f_57
lb_f_81
lb_f_91
lb_f_96
lb_s_110
lb_s_125
lb_s_129
lb_s_13
lb_s_135
lb_s_138
lb_s_148
lb_s_151
lb_s_159
lb_s_170
lb_s_171
lb_s_176
lb_s_184
lb_s_19
lb_s_204
lb_s_207
lb_s_21
lb_s_211
lb_s_226
lb_s_231
lb_s_239
lb_s_242
lb_s_245
lb_s_248
lb_s_257
lb_s_269
lb_s_280
lb_s_292
lb_s_295
lb_s_309
lb_s_313
lb_s_38
lb_s_39
lb_s_4
lb_s_41
lb_s_50
lb_s_62
lb_s_63
lb_s_72
lb_s_77
lb_s_80
lb_s_82
lb_s_90
rs_f_10
rs_f_114
rs_f_115
rs_f_120
rs_f_13
rs_f_130
rs_f_135
rs_f_137
rs_f_138
rs_f_144
rs_f_152
rs_f_157
rs_f_158
rs_f_160
rs_f_168
rs_f_178
rs_f_18
rs_f_183
rs_f_185
rs_f_186
rs_f_189
rs_f_203
rs_f_204
rs_f_205
rs_f_210
rs_f_214
rs_f_215
rs_f_217
rs_f_228
rs_f_233
rs_f_236
rs_f_240
rs_f_241
rs_f_257
rs_f_262
rs_f_263
rs_f_270
rs_f_272
rs_f_290
rs_f_31
rs_f_34
rs_f_42
rs_f_45
rs_f_46
rs_f_5
rs_f_53
rs_f_54
rs_f_66
rs_f_79
rs_f_85
rs_f_87
rs_s_10
rs_s_106
rs_s_113
rs_s_118
rs_s_12
rs_s_120
rs_s_125
rs_s_127
rs_s_136
rs_s_140
rs_s_15
rs_s_152
rs_s_164
rs_s_166
rs_s_169
rs_s_170
rs_s_172
rs_s_175
rs_s_178
rs_s_182
rs_s_188
rs_s_196
rs_s_212
rs_s_219
rs_s_223
rs_s_226
rs_s_240
rs_s_242
rs_s_244
rs_s_247
rs_s_250
rs_s_253
rs_s_258
rs_s_267
rs_s_27
rs_s_278
rs_s_283
rs_s_285
rs_s_287
rs_s_290
rs_s_291
rs_s_30
rs_s_302
rs_s_306
rs_s_31
rs_s_316
rs_s_321
rs_s_33
rs_s_331
rs_s_344
rs_s_35
rs_s_353
rs_s_356
rs_s_36
rs_s_360
rs_s_363
rs_s_369
rs_s_371
rs_s_379
rs_s_381
rs_s_383
rs_s_4
rs_s_44
rs_s_64
rs_s_65
rs_s_69
rs_s_73
rs_s_76
rs_s_8
rs_s_83
rs_s_84
rs_s_85
rs_s_87
"

#
# Align paired-end data with Bpwtie2, convert to BAM and SORT.
#

for sample in $files
do 
/opt/bio-bwa/bwa mem -t 38 ../genome/bwa_gac ../cleaned/${sample}.1.fq.gz ../cleaned/${sample}.2.fq.gz | /opt/samtools-1.6/bin/samtools view -@ 16 -bSu - | /opt/samtools-1.6/bin/samtools sort -@ 16 - -o ./${sample}.bam &> bwa_mem.oe

done

```

for all the files in a directory - does not work yet...
```{bash}
#!/bin/bash

# set max wall-clock time (D-HH:MM:SS)
#SBATCH --time=6-23:59:00

#SBATCH --cpus-per-task=38

# set partition/queue to use
#SBATCH --partition=week-long-cpu

# set name of job
#SBATCH --job-name=bwa_default

# set name of output file
#SBATCH --output=bwadefault.out

# send mail to this address
#SBATCH --mail-user=ddayan@clarku.edu

source /opt/samtools-1.6/source_me
files=find . -name "*.fq.gz" -exec basename \{} .fq.gz \; | sed 's/\.[12]//' | sed 's/.rem//' | uniq


for sample in $files
do 
/opt/bio-bwa/bwa mem -t 38 ../genome/bwa/bwa_gac ../cleaned_run2_lane8/${sample}.1.fq.gz ../cleaned_run2_lane8/${sample}.2.fq.gz | /opt/samtools-1.6/bin/samtools view -@ 16 -bSu - | /opt/samtools-1.6/bin/samtools sort -@ 16 - -o ./${sample}.bam &> bwa_mem.oe

done

```



time estimate:
  for single lane 3 hours
  
  
### gstacks

```{bash}
#!/bin/bash

# set max wall-clock time (D-HH:MM:SS)
#SBATCH --time=6-23:59:00

#SBATCH --cpus-per-task=38

# set partition/queue to use
#SBATCH --partition=week-long-cpu

# set name of job
#SBATCH --job-name=gstacks12-8

# set name of output file
#SBATCH --output=gstacks12-8.out

# mail alert at start, end and abortion of execution
#SBATCH --mail-type=ALL

# send mail to this address
#SBATCH --mail-user=ddayan@clarku.edu

source /opt/stacks-2.3/bin/source_me
/opt/stacks-2.3/bin/gstacks -I ../alignments/ -M ../info/../info/popmap_first12-8.txt --rm-pcr-duplicates -O ./ -t 38 &> gstacks_log_12-8.oe


```


### populations


```{bash}
#!/bin/bash

# set max wall-clock time (D-HH:MM:SS)
#SBATCH --time=6-23:59:00

#SBATCH --cpus-per-task=38

# set partition/queue to use
#SBATCH --partition=week-long-cpu

# set name of job
#SBATCH --job-name=popfiltfirst12-8

# set name of output file
#SBATCH --output=popfiltfirst12-8.out

# mail alert at start, end and abortion of execution
#SBATCH --mail-type=ALL

# send mail to this address
#SBATCH --mail-user=ddayan@clarku.edu

source /opt/stacks-2.3/bin/source_me
/opt/stacks-2.3/bin/populations --in-path ./ -M ../info/popmap_first12-8.txt -t 38 -r 0.95  --vcf --merge_sites -e pstI &> lane8_popfilt.oe




```

